{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Demo for Ranking\n",
    "\n",
    "This notebook demonstrates using XGBoost as a Ranking classifier. You are allowed to use and/or modify this code for the Project (Part-2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.90'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Random Features and labels for Training Data\n",
    "\n",
    "At first, we generate some random features to train the XGBoost Classifier. For the project, you will be required to use the data provided (explained in `6714_proj_part2.ipynb`) to generate your features.\n",
    "\n",
    "For this example, we assume:<br>\n",
    "\n",
    "* We have 5 mentions in the training data, with total number of candidate entities for each mention as follows: [5, 4, 4, 3, 4].\n",
    "\n",
    "* We form pairs of the form: $(mention,candidate\\_entity)$, so we will have 20 pairs (for 5 mentions) in total $\\sim \\; \\sum_{i=1}^{N}{\\#c_{m_i}}$, where $\\#c_{m_i}$ corresponds to the number of candidates of the mention $m_{i}$. We consider the candidate entities corresponding to each mention as a seperate group.\n",
    "\n",
    "* For each <mention, entity> pair, we may generate some features using men_docs ($men\\_docs.pickle$) and entity description pages ($parsed\\_candidate\\_entities.pickle$). For illustration, we randomly generate some features (d-dimensional). For 20 <mention, entity> pairs, we will have a fearure matrix of the shape $(20 \\times d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Randomly Generate Features for Training....\n",
    "\n",
    "### Set Numpy Seed\n",
    "np.random.seed(23)\n",
    "\n",
    "### We generate random features (13-dim). The feature matrix will be of the shape: (20,13)\n",
    "train_data = np.random.rand(20, 13)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels for the Training data\n",
    "\n",
    "* Next, we assign labels to each <mention,entity> pair in the training data, such that:\n",
    "> * The Ground Truth entity Label is assigned a label (1) and is positioned at the start of the group (although, strictly speaking you may place the Ground Truth label at any position within the group, we do so in order to facilitate explanation). <br>\n",
    "> * The rest of the <mention, entity> pairs are assigned a label (0).\n",
    "\n",
    "**Note:** The features generated from each <mention, entity> pair should also follow the same order as that of the labels in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Labels for training data...\n",
    "train_labels = np.array([1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups:\n",
    "\n",
    "Here, we form groups for the training data, i.e., represent total number of candidate entities corresponding to each mention in the training data. [5, 4, 4, 3, 4] means that the first mention contains 5 candidate entities, second mention contains 4 candidate entities and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 4 4 3 4]\n"
     ]
    }
   ],
   "source": [
    "## Form Groups...\n",
    "\n",
    "idxs = np.where(train_labels == 1)[0]\n",
    "train_groups = np.append(np.delete(idxs, 0), len(train_labels)) - idxs\n",
    "print(train_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Data for XGBoost...\n",
    "\n",
    "For model training, $XGBoost$ expects a `DMatrix`. Here, we transform our training data to XGboost's `DMatrix` form. For illustration, you may check-out the documentation of the $XGBoost$ classifier: https://xgboost.readthedocs.io/en/latest/python/python_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(features, groups, labels=None):\n",
    "    xgb_data = xgb.DMatrix(data=features, label=labels)\n",
    "    xgb_data.set_group(groups)\n",
    "    return xgb_data\n",
    "\n",
    "\n",
    "xgboost_train = transform_data(train_data, train_groups, train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Features for the Test data\n",
    "\n",
    "We follow the same steps, as described previously, to randomly generate some features for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.84666241 0.56116554 0.4548754  0.35217491 0.58585138 0.53574974\n",
      "  0.82745628 0.2645422  0.47837906 0.34778502 0.13447832 0.25997735\n",
      "  0.04763468]\n",
      " [0.3942933  0.37161619 0.55673292 0.89749455 0.16734589 0.40720642\n",
      "  0.65235334 0.56774908 0.56651886 0.5243551  0.60758002 0.12822787\n",
      "  0.18932563]\n",
      " [0.44118762 0.8620341  0.89698484 0.22892947 0.46628086 0.47056135\n",
      "  0.42573409 0.97000412 0.82589607 0.70491561 0.30427577 0.14910093\n",
      "  0.94134411]\n",
      " [0.77717993 0.24571739 0.71107965 0.2676849  0.86669559 0.23279542\n",
      "  0.2208466  0.02835486 0.49588516 0.72578247 0.34888512 0.25342074\n",
      "  0.81904977]\n",
      " [0.82679332 0.05006364 0.85396378 0.69767297 0.06041969 0.39429315\n",
      "  0.53446562 0.16026462 0.94110301 0.03090208 0.73155512 0.12956341\n",
      "  0.36066609]\n",
      " [0.04273941 0.63357783 0.72758712 0.69127487 0.83543403 0.94078074\n",
      "  0.82748791 0.7079669  0.5466595  0.63419059 0.89571048 0.87200407\n",
      "  0.63660716]\n",
      " [0.06100773 0.68483392 0.47766207 0.71093358 0.21140509 0.70134567\n",
      "  0.89776649 0.67944913 0.19305661 0.54069283 0.87479584 0.57738215\n",
      "  0.3292028 ]\n",
      " [0.55757905 0.17687896 0.6708299  0.86826272 0.48032512 0.35981006\n",
      "  0.94892733 0.7200288  0.30113836 0.26713229 0.59350526 0.26179553\n",
      "  0.57537396]\n",
      " [0.53769606 0.37156294 0.26315835 0.90782452 0.34448872 0.1968116\n",
      "  0.99028791 0.54478819 0.58183411 0.60502247 0.1251587  0.86580458\n",
      "  0.66019143]\n",
      " [0.05081015 0.73737067 0.58483818 0.08749118 0.17406828 0.97847183\n",
      "  0.34320863 0.59016837 0.08392991 0.5637555  0.82159856 0.45312586\n",
      "  0.3573585 ]]\n"
     ]
    }
   ],
   "source": [
    "## Randomly Generate Features for Testing....\n",
    "\n",
    "## Set Numpy Random seed...\n",
    "np.random.seed(53)\n",
    "\n",
    "## Generate features of same dimensionality as that of training features...\n",
    "test_data = np.random.rand(10, 13)\n",
    "print(test_data)\n",
    "\n",
    "## Assign Groups, assuming there are 3 mentions, with 3, 3 and 4 candidate entities...\n",
    "test_groups = np.array([3, 3, 4])\n",
    "\n",
    "# Transform the features to XGBoost DMatrix...\n",
    "xgboost_test = transform_data(test_data, test_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training + Prediction\n",
    "\n",
    "After feature generation, and data transformation, the next step is to set hyper-parameters of the $XGBoost$ classifier and and train our model. Once the model is trained, we use it to generate predictions for the testing data.\n",
    "\n",
    "**Note:** We use `rank:pairwise` as the objective function of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.0079966,  1.3935335, -0.5236801,  2.3061101,  0.3833349,\n",
       "       -1.1243987, -1.0931433,  2.9371133,  2.4778368, -0.9368963],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Parameters for XGBoost, you can fine-tune these parameters according to your settings...\n",
    "\n",
    "param = {'max_depth': 8, 'eta': 0.05, 'silent': 1, 'objective': 'rank:pairwise',\n",
    "         'min_child_weight': 0.01, 'lambda':100}\n",
    "\n",
    "## Train the classifier...\n",
    "classifier = xgb.train(param, xgboost_train, num_boost_round=4900)\n",
    "##  Predict test data...\n",
    "preds = classifier.predict(xgboost_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction scores of Each Testing Group...\n",
    "\n",
    "We can separetely consider the prediction score of each group to get the final entity corresponding to each mention. Based on the prediction scores for each group, you may select the best candidate entity for the testing mention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0079966\n",
      "Prediction scores for Group 0 = [ 2.0079966  1.3935335 -0.5236801]\n",
      "2.3061101\n",
      "Prediction scores for Group 1 = [ 2.3061101  0.3833349 -1.1243987]\n",
      "2.9371133\n",
      "Prediction scores for Group 2 = [-1.0931433  2.9371133  2.4778368 -0.9368963]\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "for iter_, group in enumerate(test_groups):\n",
    "    print(max(preds[idx:idx+group]))\n",
    "    print(\"Prediction scores for Group {} = {}\".format(iter_,preds[idx:idx+group]))\n",
    "    idx+=group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
